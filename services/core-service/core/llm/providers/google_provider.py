"""
Google Gemini Provider with Search and Maps support
"""

import asyncio
import json
import logging
import os
from typing import Any

from google import genai
from google.genai import types

from ..llm_request import LLMRequest
from ..llm_response import LLMResponse
from .llm_provider_base import LLMProviderBase
from .llm_provider_error import LLMProviderError
from .llm_provider_timeout_error import LLMProviderTimeoutError

logger = logging.getLogger(__name__)


def _get_enable_mock_llm() -> bool:
    """
    Get ENABLE_MOCK_LLM from environment or settings.
    
    Reads from environment variable ENABLE_MOCK_LLM, with fallback to settings
    if available. Defaults to False (production mode).
    
    Note: This function is used as fallback if enable_mock_llm is not passed
    via kwargs from LLMFactory.
    """
    # Try to get from environment first (for backward compatibility)
    env_value = os.getenv("ENABLE_MOCK_LLM")
    if env_value is not None:
        return env_value.lower() == "true"
    
    # Try to get from settings (Pydantic loads .env automatically)
    try:
        import sys
        import os as os_module
        sys.path.insert(0, os_module.path.join(os_module.path.dirname(__file__), '../../../..'))
        from shared.config.settings import get_core_settings
        settings = get_core_settings()
        return settings.enable_mock_llm
    except Exception:
        # Fallback to False if settings not available
        return False


class GoogleProvider(LLMProviderBase):
    """Google Gemini LLM Provider with Search and Maps support"""

    def __init__(self, api_key: str, **kwargs: Any) -> None:
        super().__init__(api_key, **kwargs)

        # Lire enable_mock_llm depuis les settings ou l'environnement
        enable_mock_llm = kwargs.get("enable_mock_llm")
        if enable_mock_llm is None:
            enable_mock_llm = _get_enable_mock_llm()
        
        self.enable_mock_llm = enable_mock_llm

        # Initialiser le client selon le mode (mock ou production)
        print(f"üîß [INIT] ENABLE_MOCK_LLM = {self.enable_mock_llm}", flush=True)
        print(f"üîß [INIT] API Key pr√©sente: {bool(api_key)}", flush=True)

        if self.enable_mock_llm:
            print("üîß [INIT] Mode MOCK activ√© - pas de connexion GCP", flush=True)
            logger.info("üîß Initialisation en mode MOCK (pas de connexion GCP)")
            self.client = None  # Mock: pas de client r√©el
        else:
            print("‚úÖ [INIT] Mode PRODUCTION - initialisation client GCP", flush=True)
            logger.info("‚úÖ Initialisation du client GCP Gemini")
            try:
                self.client = genai.Client(api_key=api_key)
                print(f"‚úÖ [INIT] Client GCP cr√©√©: {self.client is not None}", flush=True)
            except Exception as e:
                print(f"‚ùå [INIT] Erreur cr√©ation client GCP: {e}", flush=True)
                logger.error(f"Erreur cr√©ation client GCP: {e}")
                raise

        # Available models (2.5 uniquement)
        self.models = {
            "gemini-2.5-flash": "gemini-2.5-flash",
            "gemini-2.5-flash-lite": "gemini-2.5-flash-lite",
            "gemini-3-flash-preview": "gemini-3-flash-preview"
        }
        
        # Mapping des anciens noms de mod√®les vers les nouveaux
        self.model_mapping = {
            "gemini-2.5-flash": "gemini-2.5-flash",  # Migration 2.0 -> 2.5
            "gemini-2.5-flash-lite": "gemini-2.5-flash-lite",
            "gemini-3-flash-preview": "gemini-3-flash-preview"
        }

        # Default model (comme le POC)
        self.default_model = "gemini-3-flash-preview"

    async def generate(self, request: LLMRequest, max_retries: int = 3) -> LLMResponse:
        """Generate text using Google Gemini with optional Search and Maps
        
        Args:
            request: LLM request
            max_retries: Maximum number of retries for 500 errors (default: 3)
        """
        try:
            self._validate_request(request)

            model_name = request.model or self.default_model
            
            # Mapper les anciens noms de mod√®les vers les nouveaux
            if model_name in self.model_mapping:
                model_name = self.model_mapping[model_name]
                logger.info(f"Model mapped from {request.model} to {model_name}")
            
            # Si le mod√®le n'est toujours pas dans la liste, utiliser le default
            if model_name not in self.models:
                logger.warning(f"Model {model_name} not in available models, using default {self.default_model}")
                model_name = self.default_model

            # Prepare content
            if request.system_message:
                full_prompt = f"{request.system_message}\n\n{request.prompt}"
            else:
                full_prompt = request.prompt

            # üîç DEBUG MODE: Log prompt complet
            debug_mode = os.getenv("LLM_DEBUG_MODE", "false").lower() == "true"
            if debug_mode:
                logger.info("=" * 80)
                logger.info("üîç DEBUG MODE - PROMPT COMPLET")
                logger.info("=" * 80)
                logger.info(f"Provider: {self.__class__.__name__}")
                logger.info(f"Model: {model_name}")
                logger.info(f"System Message: {request.system_message[:200] if request.system_message else 'None'}...")
                logger.info(f"Prompt Length: {len(request.prompt)} chars")
                logger.info(f"\nüìù PROMPT COMPLET:\n{full_prompt}")
                logger.info("=" * 80)

            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=full_prompt)]
                )
            ]

            # ‚úÖ NOUVEAU : Configuration des tools
            tools = []
            if request.use_search:
                logger.info("üîç Enabling Google Search grounding")
                tools.append(types.Tool(google_search=types.GoogleSearch()))

            if request.use_maps:
                logger.info("üó∫Ô∏è Enabling Google Maps grounding")
                tools.append(types.Tool(google_maps=types.GoogleMaps()))

            # Configuration de g√©n√©ration (comme dans le POC)
            # Valeur par d√©faut raisonnable pour √©viter les co√ªts excessifs
            # Si max_tokens n'est pas d√©fini, on limite √† 8000 tokens au lieu de 65535
            DEFAULT_MAX_TOKENS = 8000
            generate_content_config = types.GenerateContentConfig(
                temperature=request.temperature,
                top_p=0.95,
                max_output_tokens=request.max_tokens or DEFAULT_MAX_TOKENS,
                safety_settings=[
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    ),
                    types.SafetySetting(
                        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                        threshold=types.HarmBlockThreshold.BLOCK_NONE
                    )
                ],
                tools=tools if tools else None,
            )

            # Generate content avec retry pour les erreurs 500
            loop = asyncio.get_event_loop()
            last_error = None
            
            for attempt in range(max_retries):
                try:
                    response = await loop.run_in_executor(
                        None,
                        self._generate_sync,
                        model_name,
                        contents,
                        generate_content_config
                    )
                    # Succ√®s, sortir de la boucle
                    break
                except Exception as e:
                    last_error = e
                    error_str = str(e)
                    # V√©rifier si c'est une erreur 500 ou 503 de Google (retry)
                    is_retryable = (
                        "500 INTERNAL" in error_str or 
                        "503 UNAVAILABLE" in error_str or 
                        "ServerError" in error_str
                    )
                    
                    if is_retryable:
                        if attempt < max_retries - 1:
                            # Pour 503 (overloaded), attendre plus longtemps
                            if "503" in error_str:
                                wait_time = 5 * (attempt + 1)  # 5s, 10s, 15s pour 503
                            else:
                                wait_time = 2 ** attempt  # 1s, 2s, 4s pour 500
                            
                            logger.warning(
                                f"Google API error (attempt {attempt + 1}/{max_retries}), "
                                f"retrying in {wait_time}s... Error: {error_str[:200]}"
                            )
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"Google API error after {max_retries} attempts: {error_str[:200]}")
                            raise
                    else:
                        # Autre type d'erreur, ne pas retry
                        raise

            # Parse response
            if not response or not response.text:
                logger.warning(
                    "Empty response from Google Gemini - returning fallback placeholder"
                )
                # Retourner un placeholder au lieu de lever une exception
                return LLMResponse(
                    text="[Contenu temporairement indisponible - API Gemini a retourn√© une r√©ponse vide]",
                    model=mapped_model,
                    usage={
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0
                    },
                    finish_reason="empty_response",
                    provider="google"
                )

            # Extract usage information (g√©rer les None pour gemini-3-flash-preview)
            usage = None
            if hasattr(response, 'usage_metadata'):
                prompt_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0)
                completion_tokens = getattr(response.usage_metadata, 'candidates_token_count', None)
                total_tokens = getattr(response.usage_metadata, 'total_token_count', 0)

                # Gemini 2.5 Pro peut retourner None pour completion_tokens
                if completion_tokens is None:
                    completion_tokens = 0
                if prompt_tokens is None:
                    prompt_tokens = 0
                if total_tokens is None:
                    total_tokens = 0

                usage = {
                    "prompt_tokens": int(prompt_tokens),
                    "completion_tokens": int(completion_tokens),
                    "total_tokens": int(total_tokens)
                }

            # ‚úÖ NOUVEAU : Extraire metadata grounding si pr√©sent
            grounding_metadata = None
            if (request.use_search or request.use_maps) and hasattr(response, 'grounding_metadata'):
                grounding_metadata = {
                    "grounding_support": getattr(response.grounding_metadata, 'grounding_support', None),
                    "search_queries": getattr(response.grounding_metadata, 'search_queries', []),
                    "maps_queries": getattr(response.grounding_metadata, 'maps_queries', [])
                }

            # G√©rer les cas o√π les attributs peuvent √™tre None
            candidates = getattr(response, 'candidates', [])
            if candidates is None:
                candidates = []

            safety_ratings = getattr(response, 'safety_ratings', [])
            if safety_ratings is None:
                safety_ratings = []

            return LLMResponse(
                text=response.text,
                provider="google",
                model=model_name,
                usage=usage,
                finish_reason=getattr(response, 'finish_reason', 'stop'),
                metadata={
                    "safety_ratings": safety_ratings,
                    "candidates": len(candidates),
                    "search_enabled": request.use_search,
                    "maps_enabled": request.use_maps,
                    "grounding_metadata": grounding_metadata
                }
            )

        except Exception as e:
            if isinstance(e, LLMProviderError | LLMProviderTimeoutError):
                raise
            raise LLMProviderError(
                f"Generation failed: {str(e)}",
                provider="google"
            ) from e

    def _generate_sync(
        self,
        model_name: str,
        contents: list,
        generate_content_config: types.GenerateContentConfig
    ):
        """Synchronous generation method with new API (comme le POC)"""

        # Choisir entre mode mock et mode production
        if not self.enable_mock_llm and self.client:
            # ==========================================
            # ‚úÖ MODE PRODUCTION : APPEL R√âEL GCP
            # ==========================================
            print(f"üöÄ [GCP] Appel r√©el √† GCP Gemini : {model_name}", flush=True)
            print(f"   - Temperature: {generate_content_config.temperature}", flush=True)
            print(f"   - Max tokens: {generate_content_config.max_output_tokens}", flush=True)
            print(f"   - Tools: {generate_content_config.tools}", flush=True)

            logger.info(f"üöÄ Appel r√©el √† GCP Gemini : {model_name}")
            logger.info(f"   - Temperature: {generate_content_config.temperature}")
            logger.info(f"   - Max tokens: {generate_content_config.max_output_tokens}")
            logger.info(f"   - Tools: {generate_content_config.tools}")

            response_text = ""
            chunk_count = 0
            last_chunk = None

            try:
                stream = self.client.models.generate_content_stream(
                    model=model_name,
                    contents=contents,
                    config=generate_content_config
                )

                print("   [GCP] Stream cr√©√©, it√©ration...", flush=True)

                for chunk in stream:
                    chunk_count += 1
                    last_chunk = chunk

                    # Log verbeux d√©sactiv√© pour √©viter de polluer les logs
                    # print(f"   [GCP] Chunk {chunk_count}: type={type(chunk)}, has_text={hasattr(chunk, 'text')}", flush=True)

                    if hasattr(chunk, 'text'):
                        chunk_text = chunk.text
                        # print(f"   [GCP] Chunk text: {chunk_text[:100] if chunk_text else 'EMPTY'}...", flush=True)
                        if chunk_text:
                            response_text += chunk_text
                    else:
                        logger.debug("Chunk sans attribut 'text'")

                    logger.debug(f"   Chunk {chunk_count}: {hasattr(chunk, 'text')}, has_text={chunk.text if hasattr(chunk, 'text') else 'N/A'}")

                print(f"‚úÖ [GCP] Response: {chunk_count} chunks, {len(response_text)} caract√®res", flush=True)
                logger.info(f"‚úÖ GCP Response: {chunk_count} chunks, {len(response_text)} caract√®res")

                if not response_text:
                    print(f"‚ö†Ô∏è [GCP] GCP a retourn√© {chunk_count} chunks mais texte vide - utilisation fallback", flush=True)
                    logger.warning(f"‚ö†Ô∏è GCP a retourn√© {chunk_count} chunks mais texte vide - utilisation fallback")
                    if last_chunk:
                        print(f"   Last chunk: {last_chunk}", flush=True)
                        print(f"   Candidates: {getattr(last_chunk, 'candidates', [])}", flush=True)
                        print(f"   Safety ratings: {getattr(last_chunk, 'safety_ratings', [])}", flush=True)
                        logger.warning(f"   Last chunk: {last_chunk}")
                        logger.warning(f"   Candidates: {getattr(last_chunk, 'candidates', [])}")
                        logger.warning(f"   Safety ratings: {getattr(last_chunk, 'safety_ratings', [])}")
                    
                    # ‚úÖ Utiliser un placeholder au lieu de laisser vide
                    response_text = "[Contenu temporairement indisponible - API Gemini a retourn√© une r√©ponse vide. Cela peut arriver si le prompt d√©clenche des filtres de s√©curit√© ou si l'API est surcharg√©e.]"

            except Exception as e:
                logger.error(f"‚ùå Erreur lors du streaming GCP: {str(e)}")
                raise

            # Cr√©er un objet de r√©ponse avec gestion des None
            class RealResponse:
                def __init__(self, text, last_chunk):
                    self.text = text

                    # G√©rer usage_metadata avec valeurs par d√©faut
                    if last_chunk and hasattr(last_chunk, 'usage_metadata'):
                        usage_meta = last_chunk.usage_metadata
                        # Cr√©er un objet usage_metadata s√©curis√©
                        class SafeUsageMetadata:
                            def __init__(self, um):
                                self.prompt_token_count = getattr(um, 'prompt_token_count', 0) or 0
                                self.candidates_token_count = getattr(um, 'candidates_token_count', 0) or 0
                                self.total_token_count = getattr(um, 'total_token_count', 0) or 0
                        self.usage_metadata = SafeUsageMetadata(usage_meta)
                    else:
                        self.usage_metadata = None

                    self.safety_ratings = getattr(last_chunk, 'safety_ratings', []) if last_chunk else []
                    self.candidates = getattr(last_chunk, 'candidates', []) if last_chunk else []
                    self.finish_reason = getattr(last_chunk, 'finish_reason', 'stop') if last_chunk else 'stop'
                    self.grounding_metadata = getattr(last_chunk, 'grounding_metadata', None) if last_chunk else None

            return RealResponse(response_text, last_chunk)

        # ==========================================
        # üîß MODE MOCK : R√âPONSES FACTICES
        # ==========================================
        logger.info("üîß MODE MOCK : G√©n√©ration de r√©ponse factice (GCP d√©sactiv√©)")

        # Extraire le prompt pour g√©n√©rer une r√©ponse mock contextuelle
        prompt_text = ""
        if contents and len(contents) > 0:
            if hasattr(contents[0], 'parts') and contents[0].parts:
                if hasattr(contents[0].parts[0], 'text'):
                    prompt_text = contents[0].parts[0].text

        # üÜï MOCK SP√âCIFIQUE POUR LE TH√àME baad9ac0-5da7-4c34-9f3e-73352c2cf0ad (Fournisseurs Batteries)
        THEME_BATTERIES_ID = 'baad9ac0-5da7-4c34-9f3e-73352c2cf0ad'
        prompt_lower = prompt_text.lower()
        
        # D√©tecter le th√®me batteries par mots-cl√©s sp√©cifiques (car theme_id n'est pas dans le prompt)
        theme_batteries_keywords = [
            'fournisseurs europ√©ens', 'fournisseurs europeens',
            'ul2271', 'ul 2271',
            'lfp', 'lto', 'lithium',
            'packs batterie', 'packs batterie',
            'robotique', 'agv', 'amr',
            'leclanch√©', 'tyva energie', 'varta', 'forsee power', 'saft'
        ]
        theme_batteries_detected = (
            THEME_BATTERIES_ID in prompt_text or 
            'theme_id' in str(prompt_text).lower() or
            any(keyword in prompt_lower for keyword in theme_batteries_keywords)
        )
        
        if theme_batteries_detected:
            # D√©tecter d'abord les processeurs d'analyzer (processor_number) - PRIORIT√â
            # Les processeurs d'analyzer ont des mots-cl√©s plus sp√©cifiques
            processor_number = None
            
            # Processor 1: Identification et Segmentation Fournisseurs
            if 'identification' in prompt_lower and 'segmentation' in prompt_lower:
                processor_number = 1
            # Processor 2: Conformit√© Technique et Certification
            elif (('conformit√©' in prompt_lower or 'conformite' in prompt_lower) and 
                  ('technique' in prompt_lower or 'certification' in prompt_lower)):
                processor_number = 2
            # Processor 3: Capacit√©s Co-d√©veloppement et Int√©gration
            elif (('co-d√©veloppement' in prompt_lower or 'co-developpement' in prompt_lower or 'codeveloppement' in prompt_lower) or
                  (('capacit√©s' in prompt_lower or 'capacites' in prompt_lower) and ('int√©gration' in prompt_lower or 'integration' in prompt_lower))):
                processor_number = 3
            # Processor 4: Flexibilit√© et Risques Op√©rationnels
            elif (('flexibilit√©' in prompt_lower or 'flexibilite' in prompt_lower) or
                  ('risques' in prompt_lower and ('op√©rationnels' in prompt_lower or 'operationnels' in prompt_lower or 'op√©rationnel' in prompt_lower))):
                processor_number = 4
            # Processor 5: Recommandations et Plan d'Action
            elif (('recommandations' in prompt_lower or 'recommandation' in prompt_lower) or
                  ('plan' in prompt_lower and ('action' in prompt_lower or 'd\'action' in prompt_lower))):
                processor_number = 5
            
            # D√©tecter le prompt_number (captation) seulement si aucun processor_number n'a √©t√© d√©tect√©
            prompt_number = None
            if not processor_number:
                # D√©tection par mots-cl√©s dans le prompt (ordre de priorit√©, plus sp√©cifique)
                # Prompt 1: Identification Fournisseurs Europ√©ens (mots-cl√©s sp√©cifiques)
                if (('identifier' in prompt_lower or 'identification' in prompt_lower) and 
                    ('fournisseurs' in prompt_lower or 'fournisseur' in prompt_lower) and 
                    ('europ√©ens' in prompt_lower or 'europeens' in prompt_lower or 'european' in prompt_lower) and
                    'segmentation' not in prompt_lower and
                    'production' not in prompt_lower and
                    'capacit√©s' not in prompt_lower and 'capacites' not in prompt_lower):
                    prompt_number = 1
                # Prompt 2: Capacit√©s de Production D√©taill√©es (mots-cl√©s sp√©cifiques)
                elif (('capacit√©s' in prompt_lower or 'capacites' in prompt_lower or 'capacit√©' in prompt_lower or 'capacite' in prompt_lower) and 
                      ('production' in prompt_lower) and
                      ('d√©taill√©es' in prompt_lower or 'detaillees' in prompt_lower or 'd√©taill√©e' in prompt_lower or 'detaillee' in prompt_lower or 'd√©tail' in prompt_lower or 'detail' in prompt_lower) and
                      'co-d√©veloppement' not in prompt_lower and 'co-developpement' not in prompt_lower and
                      'codeveloppement' not in prompt_lower and
                      'int√©gration' not in prompt_lower and 'integration' not in prompt_lower and
                      'approvisionnement' not in prompt_lower and 'supply chain' not in prompt_lower):
                    prompt_number = 2
                # Prompt 3: Cha√Æne d'Approvisionnement (mots-cl√©s sp√©cifiques)
                elif (('cha√Æne' in prompt_lower or 'chaine' in prompt_lower or 'chain' in prompt_lower) and 
                      ('approvisionnement' in prompt_lower or 'supply chain' in prompt_lower or 'supply' in prompt_lower) and
                      'production' not in prompt_lower and
                      'capacit√©s' not in prompt_lower and 'capacites' not in prompt_lower):
                    prompt_number = 3
                # Prompt 4: Exp√©rience Robotique (mots-cl√©s sp√©cifiques)
                elif (('robotique' in prompt_lower or 'robot' in prompt_lower) and 
                      ('exp√©rience' in prompt_lower or 'experience' in prompt_lower or 'expertise' in prompt_lower) and
                      'ul2271' not in prompt_lower and
                      'conformit√©' not in prompt_lower and 'conformite' not in prompt_lower):
                    prompt_number = 4
                # Prompt 5: Expertise UL2271 (mots-cl√©s sp√©cifiques)
                elif (('ul2271' in prompt_lower or 'ul 2271' in prompt_lower) and 
                      ('expertise' in prompt_lower or 'certification' in prompt_lower) and
                      'conformit√©' not in prompt_lower and 'conformite' not in prompt_lower and
                      'technique' not in prompt_lower):
                    prompt_number = 5
                # Prompt 6: Sant√© Financi√®re (mots-cl√©s sp√©cifiques)
                elif (('financi√®re' in prompt_lower or 'financiere' in prompt_lower or 'financier' in prompt_lower or 'financial' in prompt_lower) and 
                      ('sant√©' in prompt_lower or 'sante' in prompt_lower or 'health' in prompt_lower) and
                      'support' not in prompt_lower and
                      'services' not in prompt_lower):
                    prompt_number = 6
                # Prompt 7: Support et Services (mots-cl√©s sp√©cifiques)
                elif (('support' in prompt_lower) and 
                      ('services' in prompt_lower or 'service' in prompt_lower) and
                      'financi√®re' not in prompt_lower and 'financiere' not in prompt_lower):
                    prompt_number = 7
            
            # Log de debug pour la d√©tection
            logger.debug(f"üîç D√©tection mock: prompt_number={prompt_number}, processor_number={processor_number}, prompt_preview={prompt_text[:200] if prompt_text else 'EMPTY'}")
            
            # Charger les r√©ponses depuis les fichiers JSON si disponibles
            if prompt_number or processor_number:
                try:
                    import json
                    import os as os_module
                    
                    # Chemin vers les fichiers JSON dans le conteneur Docker
                    # Essayer d'abord /app/mocks (dans Docker), puis /tmp (local)
                    base_paths = ['/app/mocks', '/tmp', os_module.path.join(os_module.path.dirname(__file__), '../../mocks')]
                    captation_file = None
                    
                    for base_path in base_paths:
                        test_path = os_module.path.join(base_path, 'captation_results.json')
                        if os_module.path.exists(test_path):
                            captation_file = test_path
                            break
                    
                    # Charger les r√©sultats de captation si c'est un prompt
                    response_text = None
                    if prompt_number:
                        if captation_file and os_module.path.exists(captation_file):
                            with open(captation_file, 'r', encoding='utf-8') as f:
                                captation_results = json.load(f)
                            
                            # Chercher la r√©ponse correspondante
                            for key, value in captation_results.items():
                                if value.get('prompt_number') == prompt_number:
                                    response_text = value.get('response', '')
                                    if response_text:
                                        logger.info(f"üîß MOCK TH√àME BATTERIES : Retour r√©ponse prompt {prompt_number} ({value.get('title', 'N/A')}) depuis {captation_file}")
                                        break
                            
                            if not response_text:
                                # Si pas trouv√©, utiliser une r√©ponse g√©n√©rique
                                response_text = f"R√©ponse mock pour le th√®me batteries, prompt {prompt_number} (r√©ponse absente dans le fichier JSON)"
                        else:
                            # Si le fichier n'existe pas, utiliser une r√©ponse g√©n√©rique
                            response_text = f"R√©ponse mock pour le th√®me batteries, prompt {prompt_number} (fichier JSON non disponible dans {base_paths})"
                    
                    # Charger les r√©sultats d'analyzer si c'est un processeur
                    if processor_number and not response_text:
                        logger.info(f"üîç Recherche processeur {processor_number} dans analyzer_results.json")
                        analyzer_file = None
                        for base_path in base_paths:
                            test_path = os_module.path.join(base_path, 'analyzer_results.json')
                            if os_module.path.exists(test_path):
                                analyzer_file = test_path
                                logger.info(f"‚úÖ Fichier analyzer_results.json trouv√©: {analyzer_file}")
                                break
                        
                        if analyzer_file and os_module.path.exists(analyzer_file):
                            with open(analyzer_file, 'r', encoding='utf-8') as f:
                                analyzer_results = json.load(f)
                            
                            logger.info(f"üìä Fichier analyzer_results.json charg√©: {len(analyzer_results)} entr√©es")
                            
                            # Chercher la r√©ponse correspondante
                            found = False
                            for key, value in analyzer_results.items():
                                if value.get('processor_number') == processor_number:
                                    found = True
                                    response_data = value.get('response', {})
                                    # Si response est un dict (JSON structur√©), le convertir en string JSON
                                    if isinstance(response_data, dict):
                                        response_text = json.dumps(response_data, ensure_ascii=False, indent=2)
                                    elif isinstance(response_data, str):
                                        # Si c'est d√©j√† une string, l'utiliser directement
                                        response_text = response_data
                                    else:
                                        # Sinon, convertir en string
                                        response_text = str(response_data) if response_data else ''
                                    
                                    if response_text:
                                        logger.info(f"üîß MOCK TH√àME BATTERIES : Retour r√©ponse processeur {processor_number} ({value.get('title', 'N/A')}) depuis {analyzer_file}")
                                        break
                            
                            if not found:
                                logger.warning(f"‚ö†Ô∏è Processeur {processor_number} non trouv√© dans analyzer_results.json")
                            if not response_text:
                                # Si pas trouv√©, utiliser une r√©ponse g√©n√©rique
                                logger.warning(f"‚ö†Ô∏è R√©ponse vide pour processeur {processor_number}, utilisation d'une r√©ponse g√©n√©rique")
                                response_text = f"R√©ponse mock pour le th√®me batteries, processeur {processor_number} (r√©ponse absente dans le fichier JSON)"
                        else:
                            # Si le fichier n'existe pas, utiliser une r√©ponse g√©n√©rique
                            response_text = f"R√©ponse mock pour le th√®me batteries, processeur {processor_number} (fichier JSON non disponible dans {base_paths})"
                        
                except Exception as e:
                    logger.warning(f"Erreur lors du chargement du mock batteries : {e}")
                    num = prompt_number or processor_number
                    num_type = "prompt" if prompt_number else "processeur"
                    response_text = f"R√©ponse mock pour le th√®me batteries, {num_type} {num} (erreur: {str(e)})"
            else:
                # Si ni prompt_number ni processor_number n'ont √©t√© d√©tect√©s, utiliser une r√©ponse g√©n√©rique
                response_text = f"R√©ponse mock pour le th√®me batteries (prompt/processeur non reconnu: {prompt_text[:100]}...)"
            
            # Cr√©er un objet de r√©ponse simple (identique √† la version r√©elle)
            class SimpleResponse:
                def __init__(self, text):
                    self.text = text
                    self.usage_metadata = None
                    self.safety_ratings = []
                    self.candidates = []
                    self.finish_reason = "stop"
                    self.grounding_metadata = None

            return SimpleResponse(response_text)

        # ============================================
        # üÜï MOCK TH√àME FINANCEMENT STARTUP PARIS
        # ============================================
        startup_keywords = [
            'financement', 'startup', 'pr√™t', 'incubateur',
            'aide', 'bpifrance', 'deeptech', 'subvention', 'amor√ßage',
            'lev√©e de fonds', 'levier financier', 'funding'
        ]
        if any(kw in prompt_lower for kw in startup_keywords):
            logger.info("üîç MOCK: Detected startup financing theme")

            # D√©tecter g√©n√©ration de CONTENU (worker-llm) vs STRUCTURE (analysis)
            content_generation_keywords = [
                'r√©dige', 'redige', '√©cris', 'ecris', 'g√©n√®re le contenu',
                'genere le contenu', 'd√©veloppe', 'developpe', 'section',
                'paragraphe', 'chapitre suivant', 'd√©taille', 'detaille',
                'produis', 'compose', '√©labore', 'elabore'
            ]

            is_content_generation = any(kw in prompt_lower for kw in content_generation_keywords)

            # Charger le mock depuis le fichier
            response_text = None

            if is_content_generation:
                # Worker LLM g√©n√®re du contenu Markdown pour une section
                logger.info("  ‚Üí Type: Content Generation (Markdown)")

                # G√©n√©rer du Markdown mock bas√© sur le sujet d√©tect√© dans le prompt
                if 'positionnement' in prompt_lower or '√©ligibilit√©' in prompt_lower:
                    response_text = """### Positionnement Strat√©gique et √âligibilit√© Innovation

L'√©volution fulgurante des capacit√©s des Large Language Models (LLM) a pr√©cipit√© le secteur de la g√©n√©ration documentaire professionnelle vers un point d'inflexion historique. Notre startup se positionne sur cette rupture technologique en combinant LLM + Grounding Web/Maps pour produire des rapports ancr√©s dans la r√©alit√© terrain.

**Proposition de Valeur Unique :**
- Intelligence Contextuelle via Grounding : croisement donn√©es internes + signaux externes (Google Search, Maps)
- Architecture Hybride : Gemini Flash 3.0 + RAG propri√©taire garantissant pr√©cision et pertinence
- Gouvernance et Auditabilit√© : chaque section g√©n√©r√©e accompagn√©e de sources URL tra√ßables

**√âligibilit√© JEI (Jeune Entreprise Innovante) :**
Notre projet coche tous les crit√®res pour b√©n√©ficier du statut JEI avec ses avantages fiscaux :
- D√©penses R&D ‚â• 60% des charges (3 ing√©nieurs ML)
- √Çge < 8 ans (cr√©ation 2026)
- Ind√©pendance capitalistique (100% fondateurs)
- V√©ritables activit√©s R&D (LLM fine-tuning, RAG)

**Impact Fiscal Estim√© (premi√®re ann√©e) :**
- Exon√©ration cotisations sociales patronales : ~45K‚Ç¨
- Cr√©dit Imp√¥t Recherche (CIR) : 30% des d√©penses R&D ‚Üí ~80K‚Ç¨
- **Total avantages fiscaux : ~125K‚Ç¨**"""

                elif 'incubateur' in prompt_lower or 'acc√©l√©rateur' in prompt_lower or '√©cosyst√®me' in prompt_lower:
                    response_text = """### L'√âcosyst√®me Parisien : Incubateurs et Acc√©l√©rateurs

Paris concentre l'un des √©cosyst√®mes DeepTech les plus dynamiques d'Europe. Voici les incubateurs prioritaires pour une startup LLM/Data :

#### **Station F - Programme Founders**
- **Sp√©cialisation** : Scaling startups tech
- **Co√ªt** : Gratuit (s√©lection sur dossier)
- **Equity demand√©** : 0%
- **Avantages** : Acc√®s 3000m¬≤ bureaux, r√©seau 30 VCs r√©sidents (Sequoia, Accel), programmes AI/SaaS
- **KPI** : ~1000 startups h√©berg√©es, taux de lev√©e post-programme : 60%

#### **Le Camping (Google for Startups)**
- **Sponsor** : Google
- **Programme** : 6 mois gratuit
- **Perks** : $100K cr√©dits Google Cloud, mentorship Google Brain/DeepMind, acc√®s beta APIs Gemini
- **Fit** : Startups utilisant massivement GCP et APIs Google

#### **Agoranov - Sp√©cialisation Sciences**
- **Focus** : DeepTech scientifique (Sorbonne, CNRS)
- **Co√ªt** : 350‚Ç¨/mois, 0% equity
- **Atouts** : Expertise scientifique (PhD advisors), partenariats recherche, financement maturation jusqu'√† 90K‚Ç¨

**Strat√©gie recommand√©e** : Postuler simultan√©ment √† Station F (scaling) + Le Camping (cr√©dits GCP). Cumulable et synergique."""

                elif 'bpifrance' in prompt_lower or 'subvention' in prompt_lower or 'financement public' in prompt_lower:
                    response_text = """### Financements Publics et Subventions (Bpifrance & √âtat)

#### **Bourse French Tech (ex-Concours I-Lab)**
- **Montant** : 90K‚Ç¨ √† 600K‚Ç¨ (subvention non dilutive)
- **Crit√®re** : Innovation technologique issue de la recherche
- **Calendrier** : 2 appels/an (mars, septembre)
- **Taux d'acceptation** : ~15%
- **Livrables attendus** : Dossier scientifique 20 pages, pitch jury experts 15 min, preuve de concept fonctionnel

#### **Pr√™t Innovation Bpifrance (PI/PIA)**
- **Montant** : 50K‚Ç¨ √† 3M‚Ç¨
- **Taux** : 0% si √©chec du projet (!), sinon 4-6%
- **Sans garantie personnelle**
- **Conditions** : Entreprise < 5 ans, budget R&D > 20% CA, innovation technologique d√©montr√©e

#### **Plan de financement optimal Ann√©e 1**
```
Subventions Publiques:
- Bourse French Tech : 90K‚Ç¨
- Aide Maturation R√©gion : 30K‚Ç¨
- CIR (Cr√©dit Imp√¥t) : 80K‚Ç¨
Total non-dilutif : 200K‚Ç¨

Quasi-Fonds Propres:
- Pr√™t Innovation BPI : 150K‚Ç¨
- Pr√™ts d'honneur : 60K‚Ç¨
Total dette souple : 210K‚Ç¨

TOTAL : 410K‚Ç¨ (100% non-dilutif)
```"""

                else:
                    # Contenu g√©n√©rique pour autres sections
                    response_text = f"""### Financement Startup DeepTech Paris

Cette section pr√©sente les strat√©gies de financement adapt√©es aux startups DeepTech parisiennes sp√©cialis√©es dans la g√©n√©ration documentaire par LLM.

**Points cl√©s** :
- Positionnement sur un march√© en forte croissance ($12Bn+ TAM reporting BI)
- Avantages comp√©titifs : LLM fine-tuning + Grounding Web/Maps + RAG s√©curis√©
- √âligibilit√© aux dispositifs fiscaux innovants (JEI, CIR)
- Acc√®s aux meilleurs incubateurs europ√©ens (Station F, Le Camping)
- Financements publics non-dilutifs disponibles (Bpifrance, R√©gion)

**Recommandations actionnables** :
1. D√©poser dossier JEI d√®s cr√©ation (formulaire 2069-A-SD)
2. Postuler Station F + Le Camping simultan√©ment
3. Pr√©parer dossier Bourse French Tech 3 mois avant deadline
4. Structurer budget R&D pour maximiser CIR (30% des d√©penses)

**Avantages fiscaux cumul√©s premi√®re ann√©e** : ~125K‚Ç¨"""

                logger.info(f"‚úÖ Mock Markdown g√©n√©r√©: {len(response_text)} caract√®res")

            elif 'outline' in prompt_lower or 'strategic' in prompt_lower:
                # Analysis service g√©n√®re un outline JSON structur√©
                logger.info("  ‚Üí Type: Strategic Outline (JSON)")
                try:
                    import json
                    base_paths = ['/app/mocks', '/tmp', os.path.join(os.path.dirname(__file__), '../../mocks')]
                    for base_path in base_paths:
                        test_path = os.path.join(base_path, 'analyzer_results.json')
                        if os.path.exists(test_path):
                            with open(test_path, 'r', encoding='utf-8') as f:
                                mock_data = json.load(f)
                            if 'startup_financing_paris' in mock_data:
                                response_data = mock_data['startup_financing_paris'].get('response', {})
                                response_text = json.dumps(response_data, ensure_ascii=False, indent=2)
                                logger.info(f"‚úÖ Mock JSON charg√©: {len(response_text)} caract√®res")
                            break
                except Exception as e:
                    logger.warning(f"Erreur chargement outline: {e}")
                    response_text = "Erreur chargement mock outline"

            elif 'research' in prompt_lower or 'captation' in prompt_lower:
                # Captation service g√©n√®re un plan de recherche
                logger.info("  ‚Üí Type: Research Planning (JSON)")
                try:
                    import json
                    base_paths = ['/app/mocks', '/tmp', os.path.join(os.path.dirname(__file__), '../../mocks')]
                    for base_path in base_paths:
                        test_path = os.path.join(base_path, 'captation_results.json')
                        if os.path.exists(test_path):
                            with open(test_path, 'r', encoding='utf-8') as f:
                                mock_data = json.load(f)
                            if 'startup_financing_paris' in mock_data:
                                response_data = mock_data['startup_financing_paris'].get('response', {})
                                response_text = json.dumps(response_data, ensure_ascii=False, indent=2)
                                logger.info(f"‚úÖ Mock research plan charg√©: {len(response_text)} caract√®res")
                            break
                except Exception as e:
                    logger.warning(f"Erreur chargement research: {e}")
                    response_text = "Erreur chargement mock research"

            else:
                # Fallback: contenu Markdown g√©n√©rique
                logger.info("  ‚Üí Type: Generic Markdown")
                response_text = """### Strat√©gie de Financement Startup DeepTech

L'acc√®s au financement pour une startup DeepTech parisienne n√©cessite une approche structur√©e combinant aides publiques et investissement priv√©.

**Approche Recommand√©e** :
- Phase 1 : Maximiser les financements non-dilutifs (Bpifrance, CIR, JEI)
- Phase 2 : Int√©grer un incubateur de r√©f√©rence (Station F, Le Camping)
- Phase 3 : Construire traction avant lev√©e Seed

**Budget type Ann√©e 1** : 400-600K‚Ç¨ dont 60-70% non-dilutif possible."""

            # Retourner la r√©ponse
            class SimpleResponse:
                def __init__(self, text):
                    self.text = text
                    self.usage_metadata = None
                    self.safety_ratings = []
                    self.candidates = []
                    self.finish_reason = "stop"
                    self.grounding_metadata = None

            return SimpleResponse(response_text)

        # G√©n√©rer une r√©ponse mock bas√©e sur le prompt
        if "g√©olocalisation" in prompt_text.lower() or "accessibilit√©" in prompt_text.lower():
            response_text = """Analyse g√©olocalisation et accessibilit√© commerciale - R√âPONSE MOCK

**Localisation Exacte :**
- Adresse : Rue du Ventoux, 59650 Villeneuve-d'Ascq
- Coordonn√©es GPS : 50.61669000, 3.16664000
- Code postal : 59650
- Zone commerciale : Centre Commercial Auchan V2

**Accessibilit√© Transport :**
- Voiture : Acc√®s facile via Boulevard de Valmy, parking gratuit de 3050 places
- Transport public : Lignes de bus 13, 18, 32, m√©tro ligne M1 station "Villeneuve D'Ascq H√¥tel De Ville"
- Distance arr√™t : 4 minutes √† pied

**Environnement Commercial :**
- Centres commerciaux : Auchan V2, Heron Parc √† proximit√©
- Flux de passage : √âlev√©, particuli√®rement les week-ends

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "chalandise" in prompt_text.lower() or "d√©mographie" in prompt_text.lower():
            response_text = """Zone de Chalandise et D√©mographie Commerciale - R√âPONSE MOCK

**Zone Primaire (< 15 minutes) :**
- Villeneuve-d'Ascq : 62 342 habitants
- Lille : 236 710 habitants
- Mons-en-Bar≈ìul : ~22 567 habitants

**Profil D√©mographique :**
- Population totale zone primaire : ~320 000 habitants
- R√©partition par √¢ges : 18% 0-17 ans, 25% 18-34 ans, 28% 35-54 ans, 29% 55+ ans
- Composition m√©nages : Familles, couples, personnes seules (pr√©sence √©tudiante importante)

**Potentiel Commercial :**
- Zone √† forte densit√© de population active et jeune
- Pr√©sence √©tudiante importante (45 000+ √©tudiants)
- Consommation orient√©e vers la mode et les services

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "concurrentiel" in prompt_text.lower() or "concurrence" in prompt_text.lower():
            response_text = """Environnement Concurrentiel Commercial - R√âPONSE MOCK

**Concurrents Directs (Mode Masculine) :**
1. **Jules** : Mode masculine accessible, cible 25-45 ans
2. **Celio** : Mode masculine classique et d√©contract√©e
3. **Zara Men** : Mode tendance, cible 18-35 ans

**Concurrents Indirects :**
- **Decathlon** : Sportwear et mode d√©contract√©e
- **Kiabi** : Mode familiale accessible
- **C&A** : Mode familiale large

**Analyse de Positionnement :**
- Positionnement : Mode masculine accessible et tendance
- Diff√©renciation : Collections √©co-responsables, adaptation toutes morphologies
- Points forts : Conseil personnalis√©, pr√©sence centres commerciaux

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "infrastructure" in prompt_text.lower() or "√©quipements" in prompt_text.lower():
            response_text = """Infrastructure Commerciale et G√©ographie d'Usage - R√âPONSE MOCK

**√âquipements Commerciaux :**
- Centre Commercial Auchan V2 : 200+ commerces, restaurants et services
- Heron Parc : Centre commercial et de loisirs
- Parking : 3050 places gratuites

**Infrastructures :**
- Transport : M√©tro ligne 1, bus, parking gratuit
- √âducation : Universit√© de Lille, √©coles sup√©rieures
- Sant√© : CHRU Lille, cabinets m√©dicaux
- Loisirs : Cin√©ma UGC, restaurants, bars

**G√©ographie Commerciale :**
- Zone commerciale V2 : C≈ìur de l'activit√© commerciale
- Zones d'activit√©s : EuraTechnologies, Haute Borne, Parc des Moulins
- R√©sidentiel : Densit√© significative autour du centre commercial

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "√©v√©nements" in prompt_text.lower() or "flux" in prompt_text.lower():
            response_text = """√âv√©nements Commerciaux et Flux de Client√®le - R√âPONSE MOCK

**√âv√©nements Commerciaux R√©currents :**
- Halloween Circus √† Aushopping V2 : Du 22 octobre au 1er novembre 2025
- March√©s de No√´l : Playground Market (d√©cembre 2025)
- Jazz √† V√©d'A : Saison 25/26 (octobre 2025 - mai 2026)

**Flux de Client√®le :**
- √âtudiants : 45 000+ √©tudiants et chercheurs sur les campus
- Saisonnalit√© : Fortes p√©riodes en ao√ªt-septembre (rentr√©e), novembre-d√©cembre (f√™tes)
- Affluence : Pic les week-ends et p√©riodes de soldes

**Recommandations Commerciales :**
- Cibler les √©tudiants : Offres sp√©ciales rentr√©e, promotions
- √âv√©nements locaux : Participation aux march√©s de No√´l, Halloween
- Partenariats : Universit√©s, r√©sidences √©tudiantes

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "tendances" in prompt_text.lower() or "innovations" in prompt_text.lower():
            response_text = """Tendances et Innovations Mode Locales - R√âPONSE MOCK

**Tendances Mode √âmergentes :**
1. **Mode durable** : Mat√©riaux √©co-responsables, production √©thique
2. **Mode streetwear** : Continuation des tendances urbaines
3. **Mode vintage** : Retour des styles ann√©es 90-2000
4. **Personnalisation** : Adaptation √† toutes les morphologies

**√âvolutions Comportements d'Achat :**
- E-commerce : Canal essentiel, compl√©ment du magasin physique
- Personnalisation : Conseil expert, adaptation morphologie
- Location : Tendance √©mergente pour mode masculine

**Innovations Services Mode :**
- Applications de style personnel : Conseils d'experts en magasin
- Plateformes de revente : Vinted, Vestiaire Collective (impact limit√©)

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "climat" in prompt_text.lower() or "saisonnalit√©" in prompt_text.lower():
            response_text = """Climat et Saisonnalit√© des Ventes - R√âPONSE MOCK

**Profil Climatique :**
- Climat : Oc√©anique temp√©r√©
- Temp√©rature moyenne : 10-12¬∞C
- Hiver : 1-4¬∞C (tr√®s froid, venteux, nuageux)
- √ât√© : 17-18¬∞C (doux, court)

**Impact sur les Ventes :**
- Automne/Hiver : Forte demande v√™tements chauds (manteaux, pulls, bottes)
- Printemps : Transition vers tenues plus l√©g√®res
- √ât√© : V√™tements l√©gers et a√©r√©s

**Saisonnalit√© des Ventes :**
- P√©riode forte : Octobre-Mars (v√™tements chauds)
- Transition : Avril-Mai (mi-saison)
- Saison estivale : Juin-Ao√ªt (v√™tements l√©gers)

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        # ==========================================
        # üöß MOCK ANALYZER : R√©ponses pour les processeurs d'analyse
        # ==========================================
        elif ("contextes" in prompt_text.lower() or "contexte commercial" in prompt_text.lower()) or ("processor" in prompt_text.lower() and ("1" in prompt_text or "contextes" in prompt_text.lower())):
            response_text = """## 1. Positionnement G√©ographique

Le magasin Jules b√©n√©ficie d'une implantation strat√©gique au sein du Centre Commercial Auchan V2, un p√¥le majeur de la M√©tropole Europ√©enne de Lille. Cette localisation offre un acc√®s privil√©gi√© √† une zone de chalandise √©tendue gr√¢ce √† l'excellente desserte en transports en commun et au parking gratuit de 3050 places.

**Avantages :**
- Accessibilit√© optimale via m√©tro ligne 1, bus et axes autoroutiers (A1, A22, A23)
- Forte densit√© de population dans la zone primaire (< 15 minutes)
- Pr√©sence √©tudiante importante (45 000+ √©tudiants)

**Contraintes :**
- Concurrence importante dans le centre commercial
- Variation saisonni√®re des flux de client√®le

## 2. Potentiel Commercial

Le march√© pr√©sente un potentiel √©lev√© avec une population de plus de 320 000 habitants dans la zone primaire. La client√®le est diversifi√©e : jeunes actifs, √©tudiants, familles, avec un pouvoir d'achat h√©t√©rog√®ne mais globalement dynamique.

**√âvaluation quantitative :**
- Zone primaire : ~320 000 habitants
- Zone secondaire : √âlargissement jusqu'√† Roubaix, Tourcoing (population suppl√©mentaire importante)
- Pr√©sence √©tudiante : 45 000+ √©tudiants (cible privil√©gi√©e)

## 3. Concurrence

L'environnement concurrentiel est marqu√© par la pr√©sence de marques similaires (Celio, Zara Men) dans le centre commercial, n√©cessitant une diff√©renciation claire.

**Positionnement :**
- Mode masculine accessible et tendance
- Collections √©co-responsables
- Adaptation √† toutes les morphologies
- Conseil personnalis√©

## 4. Opportunit√©s

Identification de segments porteurs :
- Client√®le √©tudiante : Offres cibl√©es rentr√©e, promotions
- √âv√©nements locaux : March√©s de No√´l, Halloween Circus
- Mode durable : Tendance √©mergente forte

## 5. Risques

Facteurs limitants identifi√©s :
- Forte pluviom√©trie annuelle n√©cessitant stocks adapt√©s
- Variation saisonni√®re importante des ventes
- Sensibilit√© aux promotions et soldes

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "clients" in prompt_text.lower() and ("processor" in prompt_text.lower() or "2" in prompt_text or "segment" in prompt_text.lower()):
            response_text = """{
  "segments": [
    {
      "nom": "√âtudiants",
      "poids": 35,
      "profil": "18-25 ans, pouvoir d'achat limit√©, recherche mode accessible et tendance",
      "besoins": "Pi√®ces polyvalentes, promotions, offres √©tudiantes",
      "comportement": "Achat lors soldes, rentr√©e universitaire, √©v√©nements √©tudiants"
    },
    {
      "nom": "Jeunes Actifs",
      "poids": 30,
      "profil": "25-35 ans, pouvoir d'achat moyen, recherche √©quilibre style/prix",
      "besoins": "Tenues professionnelles d√©contract√©es, pi√®ces durables",
      "comportement": "Achat r√©gulier, fid√©lit√© aux enseignes, sensibilit√© qualit√©/prix"
    },
    {
      "nom": "Familles",
      "poids": 20,
      "profil": "35-50 ans, pouvoir d'achat variable, recherche praticit√© et durabilit√©",
      "besoins": "V√™tements fonctionnels, adapt√©s aux activit√©s familiales",
      "comportement": "Achat saisonnier, sensibilit√© aux promotions, recherche qualit√©"
    },
    {
      "nom": "Seniors Actifs",
      "poids": 15,
      "profil": "50+ ans, pouvoir d'achat moyen √† √©lev√©, recherche confort et classicisme",
      "besoins": "V√™tements adapt√©s morphologie, coupes classiques",
      "comportement": "Fid√©lit√© aux marques, recherche conseil, achat raisonn√©"
    }
  ],
  "opportunites_croissance": [
    "D√©velopper offres √©tudiantes cibl√©es",
    "Renforcer pr√©sence √©v√©nements locaux",
    "Proposer services personnalis√©s (conseil, retouches)"
  ]
}

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "quoi vendre" in prompt_text.lower() or ("produits" in prompt_text.lower() and "processor" in prompt_text.lower()) or ("3" in prompt_text and "processor" in prompt_text.lower()):
            response_text = """{
  "produits_phares": [
    {
      "categorie": "V√™tements d'hiver",
      "poids": 40,
      "produits": ["Manteaux", "Doudounes", "Pulls √©pais", "Bottes"],
      "justification": "Climat froid hiver, forte demande octobre-mars"
    },
    {
      "categorie": "V√™tements mi-saison",
      "poids": 30,
      "produits": ["Vestes l√©g√®res", "Pulls fins", "Bottines"],
      "justification": "P√©riodes transition printemps/automne importantes"
    },
    {
      "categorie": "V√™tements d'√©t√©",
      "poids": 20,
      "produits": ["T-shirts", "Shorts", "Pantalons l√©gers"],
      "justification": "Saison estivale courte mais demande pr√©sente"
    },
    {
      "categorie": "Accessoires",
      "poids": 10,
      "produits": ["√âcharpes", "Bonnets", "Gants", "Accessoires pluie"],
      "justification": "Climat pluvieux et venteux n√©cessite accessoires protecteurs"
    }
  ],
  "tendances": [
    "Mode durable : Mat√©riaux √©co-responsables",
    "Streetwear : Influence urbaine forte",
    "Personnalisation : Adaptation morphologies"
  ],
  "recommandations": [
    "Renforcer stocks v√™tements chauds pour hiver",
    "Proposer collections √©co-responsables",
    "Diversifier offre accessoires pluie"
  ]
}

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "combien" in prompt_text.lower() or ("volume" in prompt_text.lower() and "processor" in prompt_text.lower()) or ("4" in prompt_text and "processor" in prompt_text.lower()):
            response_text = """{
  "estimations_ca": {
    "ca_mensuel_moyen": "45 000 - 65 000 ‚Ç¨",
    "ca_annuel_estime": "540 000 - 780 000 ‚Ç¨",
    "methodologie": "Bas√© sur surface magasin, positionnement, zone de chalandise"
  },
  "indicateurs_cles": {
    "panier_moyen": "35 - 50 ‚Ç¨",
    "frequence_visite": "2-3 fois par mois (client√®le fid√®le)",
    "taux_rotation": "4-6 fois par an"
  },
  "variations_saisonnieres": {
    "periode_forte": "Octobre - Mars : +20% vs moyenne",
    "periode_ete": "Juin - Ao√ªt : -10% vs moyenne",
    "soldes": "Janvier, Juillet : +40% vs moyenne"
  },
  "objectifs_croissance": {
    "objectif_an_1": "+10% CA",
    "objectif_an_2": "+15% CA",
    "leviers": ["Offres √©tudiantes", "√âv√©nements locaux", "E-commerce"]
  }
}

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "comment" in prompt_text.lower() and ("processor" in prompt_text.lower() or "strat√©gie" in prompt_text.lower() or "5" in prompt_text):
            response_text = """{
  "strategie_vente": {
    "approche": "Mix online/offline avec conseil personnalis√©",
    "positionnement": "Mode masculine accessible, tendance, √©co-responsable"
  },
  "canaux_distribution": [
    {
      "canal": "Magasin physique",
      "poids": 70,
      "avantages": "Conseil expert, essayage, exp√©rience client"
    },
    {
      "canal": "E-commerce",
      "poids": 30,
      "avantages": "Confort, livraison rapide, compl√©mentarit√© magasin"
    }
  ],
  "modalites_vente": [
    "Prix fixes avec promotions saisonni√®res",
    "Programme fid√©lit√©",
    "Offres √©tudiantes (-10%)",
    "Services : retouches, conseils personnalis√©s"
  ],
  "communication": [
    "R√©seaux sociaux : Instagram, Facebook",
    "Partenariats : Universit√©s, √©v√©nements locaux",
    "Newsletter : Promotions, nouveaut√©s"
  ]
}

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "partenaires" in prompt_text.lower() and ("processor" in prompt_text.lower() or "6" in prompt_text):
            response_text = """{
  "partenaires_strategiques": [
    {
      "type": "Universit√©s",
      "partenaires": ["Universit√© de Lille", "√âcoles d'ing√©nieurs"],
      "actions": ["Offres √©tudiantes", "√âv√©nements campus", "Sponsoring"],
      "impact": "Acc√®s √† 45 000+ √©tudiants, fid√©lisation jeune client√®le"
    },
    {
      "type": "√âv√©nements locaux",
      "partenaires": ["March√©s de No√´l", "Halloween Circus", "Jazz √† V√©d'A"],
      "actions": ["Participation stands", "Promotions √©v√©nementielles"],
      "impact": "Visibilit√© locale, acc√®s nouveaux clients"
    },
    {
      "type": "Centres commerciaux",
      "partenaires": ["Aushopping V2", "Heron Parc"],
      "actions": ["Op√©rations commerciales conjointes", "Communication partag√©e"],
      "impact": "Flux client√®le mutualis√©, synergie commerciale"
    }
  ],
  "opportunites_nouveaux_partenariats": [
    "R√©sidences √©tudiantes : Offres exclusives",
    "Associations sportives : Partenariats √©quipementiers",
    "Influencers locaux : Partenariats visibilit√©"
  ]
}

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        elif "actions" in prompt_text.lower() and ("processor" in prompt_text.lower() or "7" in prompt_text or "plan d'action" in prompt_text.lower()):
            response_text = """{
  "actions_prioritaires": [
    {
      "priorite": "Haute",
      "action": "D√©velopper programme fid√©lit√© √©tudiant",
      "delai": "1 mois",
      "impact": "Fid√©lisation client√®le jeune, +15% fr√©quentation"
    },
    {
      "priorite": "Haute",
      "action": "Renforcer stocks v√™tements hiver (octobre-mars)",
      "delai": "2 semaines",
      "impact": "R√©pondre forte demande saisonni√®re, +20% CA hiver"
    },
    {
      "priorite": "Moyenne",
      "action": "Participer √©v√©nements locaux (march√©s No√´l, Halloween)",
      "delai": "2 mois",
      "impact": "Visibilit√© locale, nouveaux clients, +10% CA √©v√©nementiel"
    },
    {
      "priorite": "Moyenne",
      "action": "Lancer collection √©co-responsable",
      "delai": "3 mois",
      "impact": "Diff√©renciation concurrentielle, attractivit√© client√®le consciente"
    }
  ],
  "actions_moyen_terme": [
    "D√©velopper e-commerce avec click & collect",
    "Partenariats universit√©s pour offres exclusives",
    "Services conseil personnalis√© renforc√©"
  ]
}

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        else:
            # R√©ponse mock g√©n√©rique
            response_text = f"""R√©ponse Mock - D√©veloppement

Vous avez demand√© une analyse concernant : {prompt_text[:100]}...

Cette r√©ponse est g√©n√©r√©e automatiquement en mode d√©veloppement sans faire appel √† l'API Google Gemini.

**Informations Mock :**
- Mod√®le utilis√© : {model_name}
- Longueur du prompt : {len(prompt_text)} caract√®res
- Search enabled : {getattr(generate_content_config, 'tools', None) is not None}

Pour obtenir une vraie r√©ponse LLM, activez l'appel r√©el √† GCP dans le code.

‚ö†Ô∏è **NOTE : Cette r√©ponse est un mock g√©n√©r√© en mode d√©veloppement. Le vrai appel √† GCP est d√©sactiv√©.**"""

        # Cr√©er un objet de r√©ponse simple (identique √† la version r√©elle)
        class SimpleResponse:
            def __init__(self, text):
                self.text = text
                self.usage_metadata = None
                self.safety_ratings = []
                self.candidates = []
                self.finish_reason = "stop"
                self.grounding_metadata = None

        return SimpleResponse(response_text)

    async def health_check(self) -> bool:
        """Check if Google Gemini is accessible"""
        try:
            # Simple health check with minimal request
            test_request = LLMRequest(
                prompt="Hello",
                provider="google",
                model="gemini-1.5-flash",
                max_tokens=10
            )

            # Set timeout for health check
            response = await asyncio.wait_for(
                self.generate(test_request),
                timeout=10.0
            )

            return bool(response.text)

        except Exception:
            return False

    def get_available_models(self) -> list[str]:
        """Get available Google Gemini models"""
        return list(self.models.keys())
